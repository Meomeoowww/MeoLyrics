{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theme analysis in Rap Songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After going through my course content on Natural language processing I decided to give a try.\n",
    "We willl train on Rap Songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "* Build a list of rappers (might be by coast or other)\n",
    "\n",
    "* Build a webscrapper to extract lyrics form Genius using their api\n",
    "\n",
    "* Natural langage processing with collected data\n",
    "\n",
    "    1- use Tfidfvectorizer to fit transform and create a sparse matrix (tfidf)\n",
    "    \n",
    "    2- construct NMF based on sparse matrix to analyze thematics\n",
    "    \n",
    "    3- (optional) use kmeans to identify clusters \n",
    "        a- build pipeline with svd and kmeans\n",
    "        b- fit and predict on sparse matrix labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib3\n",
    "from urllib.parse import quote\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from io import StringIO\n",
    "\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a list of rappers (might be by coast or other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100s'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all recognized hip hop artists from wikipedia\n",
    "\n",
    "init_lib2 = urllib3.PoolManager()\n",
    "#wp2 = init_lib2.request('GET', 'https://en.wikipedia.org/wiki/List_of_hip_hop_musicians', assert_same_host=False)\n",
    "wp2 = init_lib2.request('GET', 'https://en.wikipedia.org/wiki/List_of_West_Coast_hip_hop_artists')\n",
    "html2 = BeautifulSoup(wp2.data, \"html.parser\") # Extract the page's HTML as a string\n",
    "\n",
    "b = \"\"\n",
    "for i in range(0,27):\n",
    "    a = html2.find_all(\"div\", class_=\"div-col columns column-width\")[i].get_text()\n",
    "    b = b + a\n",
    "\n",
    "rappers = pd.read_csv(StringIO(b), sep='\\t', header=None)\n",
    "\n",
    "#flatten rappers list\n",
    "k = rappers.values.tolist()\n",
    "flatrap = []\n",
    "for n1 in k:\n",
    "    for n2 in n1:\n",
    "        flatrap.append(n2)\n",
    "flatrap[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a webscrapper to extract lyrics form Genius using their api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client key\n",
    "cl_TK2 = 'X7D7eCcQjbLn9_clVs0mzhB-vg3bQv3-30zmDVVRbJW07w6m0YQHhAdSUvvJo9BV'\n",
    "base_url = \"https://api.genius.com/\"\n",
    "gen_search = \"search?q=\"\n",
    "art_search = \"artists/\"\n",
    "songs_search = \"songs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define url scrapper from genius api\n",
    "def search_title(search):\n",
    "    #build query\n",
    "    req = base_url + gen_search + quote(search) # build str query\n",
    "    http = urllib3.PoolManager() # instantiate a poolmanager\n",
    "    request = http.request('GET', req, headers={\"Authorization\":\"Bearer \" + cl_TK2, \"User-Agent\": \"\"}) # request with heqder\n",
    "\n",
    "    data = json.loads(request.data.decode('utf-8'))\n",
    "        \n",
    "    title = []\n",
    "    for i in range(0, len(data['response']['hits'])):\n",
    "        title.append(data['response']['hits'][i]['result']['title'])\n",
    "    \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define url scrapper from genius api\n",
    "def search_urls(search):\n",
    "    #build query\n",
    "    req = base_url + gen_search + quote(search) # build str query\n",
    "    http = urllib3.PoolManager() # instantiate a poolmanager\n",
    "    request = http.request('GET', req, headers={\"Authorization\":\"Bearer \" + cl_TK2, \"User-Agent\": \"\"}) # request with heqder\n",
    "\n",
    "    data = json.loads(request.data.decode('utf-8'))\n",
    "        \n",
    "    url = []\n",
    "    for i in range(0, len(data['response']['hits'])):\n",
    "        url.append(data['response']['hits'][i]['result']['url'])\n",
    "    \n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lyrics scrapper based on url\n",
    "def search_lyrics(url):    \n",
    "    # collect lyrics    \n",
    "    init_lib = urllib3.PoolManager()\n",
    "    wp = init_lib.request('GET', url)\n",
    "    html = BeautifulSoup(wp.data, \"html.parser\") # Extract the page's HTML as a string\n",
    "\n",
    "    # Scrape lyrics from webpage\n",
    "    lyrics = html.find(\"div\", class_=\"lyrics\").get_text()\n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geniusscrapper(search):\n",
    "    r = search_urls(search)\n",
    "    \n",
    "    scrap = []\n",
    "    for i, j in enumerate(r):\n",
    "        l = search_lyrics(j)\n",
    "        scrap.append(l)\n",
    "    return scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial with Kendrick Lamar\n",
    "#kdot = geniusscrapper('Eminem')\n",
    "#print(ktit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = []\n",
    "songtitle = []\n",
    "art = []\n",
    "for r, n in enumerate(flatrap):\n",
    "    \n",
    "    a = geniusscrapper(n)\n",
    "    b = search_title(n)\n",
    "    \n",
    "    art.extend(n)\n",
    "    db.extend(a)\n",
    "    songtitle.extend(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6085\n",
      "6085\n",
      "6336\n"
     ]
    }
   ],
   "source": [
    "print(len(db))\n",
    "print(len(songtitle))\n",
    "print(len(art))\n",
    "\n",
    "adb = pd.DataFrame(db)\n",
    "asongtitle = pd.DataFrame(songtitle)\n",
    "aart = pd.DataFrame(art)\n",
    "\n",
    "adb.to_csv('adb.csv', index=True)\n",
    "asongtitle.to_csv('asongtitle.csv', index=True)\n",
    "aart.to_csv('aart.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
